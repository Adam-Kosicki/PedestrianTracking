{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae211bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52760ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tracks(centers, patience):\n",
    "    \"\"\"Function to filter track history\"\"\"\n",
    "    filter_dict = {}\n",
    "    for k, i in centers.items():\n",
    "        d_frames = i.items()\n",
    "        filter_dict[k] = dict(list(d_frames)[-patience:])\n",
    "    return filter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6eed813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):\n",
    "    \"\"\"Function to update track of objects\"\"\"\n",
    "    is_new = 0\n",
    "    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]\n",
    "    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]\n",
    "    # Calculating distance from existing centers points\n",
    "    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]\n",
    "    # if distance less than a threshold, it will update its positions\n",
    "    if previous_pos:\n",
    "        id_obj = previous_pos[0][0]\n",
    "        centers_old[id_obj][frame] = obj_center\n",
    "    # Else a new ID will be set to the given object\n",
    "    else:\n",
    "        if lastKey:\n",
    "            last = lastKey.split('D')[1]\n",
    "            id_obj = 'ID' + str(int(last)+1)\n",
    "        else:\n",
    "            id_obj = 'ID0'\n",
    "        is_new = 1\n",
    "        centers_old[id_obj] = {frame:obj_center}\n",
    "        lastKey = list(centers_old.keys())[-1]\n",
    "    return centers_old, id_obj, is_new, lastKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495929ed",
   "metadata": {},
   "source": [
    "## Generate Text File w/ Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f13cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pedestrians(video_path, target_dir):\n",
    "    \n",
    "    ### Configurations #Verbose during prediction\n",
    "    verbose = False\n",
    "    # Scaling percentage of original frame\n",
    "    scale_percent = 100\n",
    "    # model confidence level\n",
    "    conf_level = 0.3\n",
    "    # Threshold of centers ( old\\new)\n",
    "    thr_centers = 30\n",
    "    # Number of max frames to consider a object lost\n",
    "    frame_max = 50\n",
    "    # Number of max tracked centers stored\n",
    "    patience = 100\n",
    "    # ROI area color transparency\n",
    "    alpha = 0.3\n",
    "    # ------------------------------------------------------- # Reading video with cv2\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Objects to detect Yolo\n",
    "    class_IDS = [0, 1] # person and bike\n",
    "    # Auxiliary variables\n",
    "    centers_old = {}\n",
    "\n",
    "    obj_id = 0\n",
    "    end = []\n",
    "#     frames_list = []\n",
    "    count_p = 0\n",
    "    lastKey = ''\n",
    "    print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "    # Original information of video\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print('[INFO] - Original Dim: ', (width, height, fps))\n",
    "\n",
    "    # Scaling Video for better performance\n",
    "    print(scale_percent)\n",
    "    if scale_percent != 100:\n",
    "        print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "        width = int(width * scale_percent / 100)\n",
    "        height = int(height * scale_percent / 100)\n",
    "        print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    print(scale_percent)\n",
    "\n",
    "    if '/' in video_path:\n",
    "        video_name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        video_name = video_path.split(\".\")[0]\n",
    "\n",
    "#     result_video_name = video_name + \".mp4\"\n",
    "    result_directory = target_dir\n",
    "#     annotated_video = result_directory + \"/Annotated_\" + result_video_name\n",
    "#     VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "#     output_video = cv2.VideoWriter(annotated_video,\n",
    "#                                    cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n",
    "#                                    fps, (width, height))\n",
    "    \n",
    "    dict_classes = model.model.names\n",
    "    # rois = extract_roi_from_video(video_path=video_path, regions=regions)\n",
    "    # roi_counts = {roi['name']: 0 for roi in rois}\n",
    "    count_p_roi = 0\n",
    "    \n",
    "    # Keep track of when objects were detected\n",
    "    object_time_tracker = {}\n",
    "    \n",
    "    for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):  # Outer loop iterating through each frame\n",
    "        # print(i)\n",
    "        # _, frame = video.read()\n",
    "\n",
    "        isFrame, frame = video.read()\n",
    "\n",
    "        if not isFrame:\n",
    "            break\n",
    "\n",
    "        # for roi in rois:  # Inner loop iterating through each region of interest\n",
    "\n",
    "        #     area_roi = [np.array(roi['polygon'], dtype=np.int32)]\n",
    "\n",
    "        # x_range, y_range = roi['range']\n",
    "        # ROI = frame[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "\n",
    "        if verbose:\n",
    "            print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # y_hat = model.predict(frame, conf=conf_level, classes=class_IDS, device='cpu', verbose=False, tracker=\"bytetrack.yaml\")\n",
    "        y_hat = model.track(frame, persist=True, conf=conf_level, classes=class_IDS, iou=0.5, show=False, verbose=False, tracker=\"bytetrack.yaml\")\n",
    "\n",
    "        boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "        conf = y_hat[0].boxes.conf.cpu().numpy()\n",
    "        classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "        positions_frame = pd.DataFrame(\n",
    "            np.concatenate(\n",
    "                [boxes, conf.reshape(-1, 1), classes.reshape(-1, 1)]\n",
    "                , axis=1\n",
    "            ),\n",
    "            columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class']\n",
    "        )\n",
    "\n",
    "        labels = [dict_classes[i] for i in classes]\n",
    "\n",
    "        for ix, row in enumerate(positions_frame.iterrows()):\n",
    "            xmin, ymin, xmax, ymax, confidence, category, = row[1].astype('int')\n",
    "            center_x, center_y = int(((xmax + xmin)) / 2), int((ymax + ymin) / 2)\n",
    "\n",
    "            centers_old, id_obj, is_new, lastKey = update_tracking(\n",
    "                centers_old,\n",
    "                (center_x, center_y),\n",
    "                thr_centers,\n",
    "                lastKey,\n",
    "                i, frame_max\n",
    "            )\n",
    "            \n",
    "            # Save time stamps of objects in frame\n",
    "            curr_frame_num = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            obj_idx = str(dict_classes[category]) + \"_\" + str(id_obj)\n",
    "            if(not(obj_idx in object_time_tracker)):\n",
    "                object_time_tracker[obj_idx] = str(datetime.timedelta(seconds=curr_frame_num/fps))\n",
    "            \n",
    "            # roi_counts[roi['name']] += is_new\n",
    "\n",
    "            # Draw rectangle around object (person, bicycle, etc...)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "            # for center_x, center_y in centers_old[id_obj].values():\n",
    "            #     cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1) # Adds circle in person\n",
    "\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=id_obj + ':' + str(np.round(conf[ix], 2)),\n",
    "                org=(xmin, ymin - 10),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                fontScale=0.8,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1\n",
    "            )\n",
    "\n",
    "        # # Update count for the current ROI in the dictionary\n",
    "        # # roi_counts[roi['name']] = count_p_roi\n",
    "        # y_coordinate = 40\n",
    "        # for region, person_count in roi_counts.items():\n",
    "        #     cv2.putText(img=frame, text=f'Counts People in ROI {region}:{person_count}',\n",
    "        #                 org=(30, y_coordinate), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #                 fontScale=1, color=(255, 0, 0), thickness=1)\n",
    "        #     y_coordinate += 50\n",
    "\n",
    "        centers_old = filter_tracks(centers_old, patience)\n",
    "        # if verbose:\n",
    "        #     print(counter_in, counter_out)\n",
    "\n",
    "        overlay = frame.copy()\n",
    "        # cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        # cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "#         frames_list.append(frame)\n",
    "#         output_video.write(frame)\n",
    "        \n",
    "#     print(object_time_tracker)\n",
    "    obj_tracker_file_ptr = open(result_directory + \"/\" + video_name+\"_obj_timestamps\", \"w\")\n",
    "    obj_tracker_file_ptr.write(str(object_time_tracker))\n",
    "    obj_tracker_file_ptr.close()\n",
    "\n",
    "#     output_video.release()\n",
    "\n",
    "    # for region, person_count in roi_counts.items():\n",
    "    #     print(f\"Number of persons moving {region} is {person_count}\")\n",
    "#     print(f\"Annotated video saved at {annotated_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add86275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n",
      "[INFO] - Verbose during Prediction: False\n",
      "[INFO] - Original Dim:  (2560, 1440, 24.95034939315925)\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2226/2226 [01:16<00:00, 29.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Device to run in\n",
    "device = None\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"using\", device, \"device\")\n",
    "\n",
    "model = YOLO('yolov9e-seg.pt')\n",
    "# use gpu for model\n",
    "model.to(device)\n",
    "# torch.cuda.synchronize()\n",
    "\n",
    "# Run\n",
    "# detect_pedestrians(\"/home/servicer/Documents/ouput_video.mp4\", \"/home/servicer/Documents\")\n",
    "detect_pedestrians(\"/home/servicer/Documents/ouput_video1.mp4\", \"/home/servicer/Documents\")\n",
    "# detect_pedestrians(\"./sources/ped_sample_vid.mp4\", \"./sources\")\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b128bac",
   "metadata": {},
   "source": [
    "## Display Pedestrians Detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184beee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pedestrians(video_path, target_dir):\n",
    "    \n",
    "    ### Configurations #Verbose during prediction\n",
    "    verbose = False\n",
    "    # Scaling percentage of original frame\n",
    "    scale_percent = 100\n",
    "    # model confidence level\n",
    "    conf_level = 0.3\n",
    "    # Threshold of centers ( old\\new)\n",
    "    thr_centers = 30\n",
    "    # Number of max frames to consider a object lost\n",
    "    frame_max = 30\n",
    "    # Number of max tracked centers stored\n",
    "    patience = 100\n",
    "    # ROI area color transparency\n",
    "    alpha = 0.3\n",
    "    # ------------------------------------------------------- # Reading video with cv2\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Objects to detect Yolo\n",
    "    class_IDS = [0, 1] # person and bike\n",
    "    # Auxiliary variables\n",
    "    centers_old = {}\n",
    "\n",
    "    obj_id = 0\n",
    "    end = []\n",
    "    count_p = 0\n",
    "    lastKey = ''\n",
    "    print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "    # Original information of video\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print('[INFO] - Original Dim: ', (width, height, fps))\n",
    "\n",
    "    # Scaling Video for better performance\n",
    "    print(scale_percent)\n",
    "    if scale_percent != 100:\n",
    "        print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "        width = int(width * scale_percent / 100)\n",
    "        height = int(height * scale_percent / 100)\n",
    "        print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    print(scale_percent)\n",
    "\n",
    "    if '/' in video_path:\n",
    "        video_name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        video_name = video_path.split(\".\")[0]\n",
    "\n",
    "    result_video_name = video_name + \".mp4\"\n",
    "    result_directory = target_dir\n",
    "    annotated_video = result_directory + \"/Annotated_\" + result_video_name\n",
    "    VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "    output_video = cv2.VideoWriter(annotated_video,\n",
    "                                   cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n",
    "                                   fps, (width, height))\n",
    "    \n",
    "    dict_classes = model.model.names\n",
    "    # rois = extract_roi_from_video(video_path=video_path, regions=regions)\n",
    "    # roi_counts = {roi['name']: 0 for roi in rois}\n",
    "    count_p_roi = 0\n",
    "    \n",
    "    # Keep track of when objects were detected\n",
    "    object_time_tracker = {}\n",
    "    \n",
    "    for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):  # Outer loop iterating through each frame\n",
    "        # print(i)\n",
    "        # _, frame = video.read()\n",
    "\n",
    "        isFrame, frame = video.read()\n",
    "\n",
    "        if not isFrame:\n",
    "            break\n",
    "\n",
    "        # for roi in rois:  # Inner loop iterating through each region of interest\n",
    "\n",
    "        #     area_roi = [np.array(roi['polygon'], dtype=np.int32)]\n",
    "\n",
    "        # x_range, y_range = roi['range']\n",
    "        # ROI = frame[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "\n",
    "        if verbose:\n",
    "            print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # y_hat = model.predict(frame, conf=conf_level, classes=class_IDS, device='cpu', verbose=False, tracker=\"bytetrack.yaml\")\n",
    "        y_hat = model.track(frame, persist=True, conf=conf_level, classes=class_IDS, iou=0.5, show=False, verbose=False, tracker=\"bytetrack.yaml\")\n",
    "        \n",
    "#         # DISPLAY ###############\n",
    "#         annotated_frame = y_hat[0].plot()\n",
    "#         cv2.imshow('YOLOv9 Tracking', annotated_frame)\n",
    "\n",
    "#         # Press 'q' to exit\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#         ########################\n",
    "        \n",
    "        boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "        conf = y_hat[0].boxes.conf.cpu().numpy()\n",
    "        classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "        positions_frame = pd.DataFrame(\n",
    "            np.concatenate(\n",
    "                [boxes, conf.reshape(-1, 1), classes.reshape(-1, 1)]\n",
    "                , axis=1\n",
    "            ),\n",
    "            columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class']\n",
    "        )\n",
    "\n",
    "        labels = [dict_classes[i] for i in classes]\n",
    "\n",
    "        for ix, row in enumerate(positions_frame.iterrows()):\n",
    "            xmin, ymin, xmax, ymax, confidence, category, = row[1].astype('int')\n",
    "            center_x, center_y = int(((xmax + xmin)) / 2), int((ymax + ymin) / 2)\n",
    "\n",
    "            centers_old, id_obj, is_new, lastKey = update_tracking(\n",
    "                centers_old,\n",
    "                (center_x, center_y),\n",
    "                thr_centers,\n",
    "                lastKey,\n",
    "                i, frame_max\n",
    "            )\n",
    "            \n",
    "            # Save time stamps of objects in frame\n",
    "            curr_frame_num = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            obj_idx = str(dict_classes[category]) + \"_\" + str(id_obj)\n",
    "            if(not(obj_idx in object_time_tracker)):\n",
    "                object_time_tracker[obj_idx] = str(datetime.timedelta(seconds=curr_frame_num/fps))\n",
    "            \n",
    "            # roi_counts[roi['name']] += is_new\n",
    "\n",
    "            # Draw rectangle around object (person, bicycle, etc...)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "            # for center_x, center_y in centers_old[id_obj].values():\n",
    "            #     cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1) # Adds circle in person\n",
    "\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=id_obj + ':' + str(np.round(conf[ix], 2)),\n",
    "                org=(xmin, ymin - 10),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                fontScale=0.8,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1\n",
    "            )\n",
    "\n",
    "        # # Update count for the current ROI in the dictionary\n",
    "        # # roi_counts[roi['name']] = count_p_roi\n",
    "        # y_coordinate = 40\n",
    "        # for region, person_count in roi_counts.items():\n",
    "        #     cv2.putText(img=frame, text=f'Counts People in ROI {region}:{person_count}',\n",
    "        #                 org=(30, y_coordinate), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #                 fontScale=1, color=(255, 0, 0), thickness=1)\n",
    "        #     y_coordinate += 50\n",
    "\n",
    "        centers_old = filter_tracks(centers_old, patience)\n",
    "        # if verbose:\n",
    "        #     print(counter_in, counter_out)\n",
    "\n",
    "        overlay = frame.copy()\n",
    "        # cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        # cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "        output_video.write(frame)\n",
    "        \n",
    "#     print(object_time_tracker)\n",
    "    obj_tracker_file_ptr = open(result_directory + \"/\" + video_name+\"_obj_timestamps\", \"w\")\n",
    "    obj_tracker_file_ptr.write(str(object_time_tracker))\n",
    "    obj_tracker_file_ptr.close()\n",
    "\n",
    "    output_video.release()\n",
    "\n",
    "    # for region, person_count in roi_counts.items():\n",
    "    #     print(f\"Number of persons moving {region} is {person_count}\")\n",
    "#     print(f\"Annotated video saved at {annotated_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe28b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Verbose during Prediction: False\n",
      "[INFO] - Original Dim:  (2560, 1440, 24.95034939315925)\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2226/2226 [02:01<00:00, 18.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Device to run in\n",
    "device = None\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"using\", device, \"device\")\n",
    "\n",
    "model = YOLO('yolov9e-seg.pt')\n",
    "# use gpu for model\n",
    "model.to(device)\n",
    "# torch.cuda.synchronize()\n",
    "\n",
    "# Run\n",
    "# display_pedestrians(\"/home/servicer/Documents/ouput_video.mp4\", \"/home/servicer/Documents\")\n",
    "display_pedestrians(\"/home/servicer/Documents/ouput_video1.mp4\", \"/home/servicer/Documents\")\n",
    "# display_pedestrians(\"./sources/ped_sample_vid.mp4\", \"./sources\")\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f7a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated_ouput_video1.mp4    ouput_video_1.mp4_obj_timestamps\r\n",
      "Annotated_ouput_video.mp4     ouput_video1_obj_timestamps\r\n",
      "Annotated_ped_sample_vid.mp4  ouput_video.mp4\r\n",
      "input.mp4\t\t      ouput_video.mp4_obj_timestamps\r\n",
      "ouput_video1.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/servicer/Documents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4aa0034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
